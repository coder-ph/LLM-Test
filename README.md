# AI-Powered Q&A System

## Overview
This is a modern, interactive full-stack web application that serves as an AI-powered Q&A system. It allows users to ask questions and receive structured, well-formatted responses generated by a Large Language Model (LLM).

The application is built to be robust and scalable, demonstrating best practices in full-stack development, API design, and containerization.

## Live Deployment

The frontend is currently deployed on Railway and can be accessed at:  
ðŸ”— [https://llm-frontend-production-cf23.up.railway.app/](https://llm-frontend-production-cf23.up.railway.app/)

The backend API is hosted separately on Render at:  
ðŸ”— [https://llm-app-latest.onrender.com](https://llm-app-latest.onrender.com)

## Features
- **Interactive Web Interface**: A clean, minimalist, and fully responsive user interface built with Next.js and styled with TailwindCSS.
- **Real-time AI Responses**: User queries are sent to a live LLM and responses are displayed in real-time.
- **Conversation History**: A session-based history feature that groups queries and responses into conversations, allowing users to revisit past chats.
- **API Integration**: Secure and efficient API endpoints for handling user queries and retrieving conversation history.
- **Swagger Documentation**: Automatically generated and interactive API documentation for easy testing and exploration.
- **Containerized Development**: The entire application stack (backend, frontend, database) is containerized using Docker Compose for a consistent and portable development environment.

## Technical Stack

### Backend
- **Language**: Python 3.12
- **Framework**: FastAPI
- **API**: RESTful API with CORS middleware
- **Database**: PostgreSQL with asyncpg driver
- **ORM**: SQLAlchemy 2.0+
- **Migrations**: Alembic
- **LLM**: Gemini API

### Frontend
- **Framework**: Next.js
- **Language**: TypeScript
- **Styling**: TailwindCSS
- **Iconography**: Lucide React
- **Other**: react-markdown, react-syntax-highlighter

## Local Development Setup
To get the application up and running locally, you need to have Docker and Docker Compose installed.

### Prerequisites
- Docker Desktop (or Docker Engine on Linux)
- A `docker-compose.yml` file configured for your local environment
- A `.env` file in your project's root directory

### `docker-compose.yml`
This file defines all the services (backend, frontend, database) and how they communicate.

```yaml
services:
  db:
    image: postgres:16-alpine
    container_name: llm_postgres
    restart: always
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  app:
    build: .
    container_name: llm_app
    command: sh -c "alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload"
    volumes:
      - .:/app
    depends_on:
      - db
    env_file:
      - .env
    environment:
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      GEMINI_API_KEY: ${GEMINI_API_KEY}
    ports:
      - "8000:8000"

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: llm_frontend
    depends_on:
      - app
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    env_file:
      - .env
    environment:
      NODE_ENV: development
      NEXT_PUBLIC_API_URL: http://localhost:8000
      PORT: 3000

volumes:
  pgdata:
